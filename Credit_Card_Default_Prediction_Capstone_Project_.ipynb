{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preritp2/Credit-Card-Default-Prediction-Supervised-ML---Classification-/blob/main/Credit_Card_Default_Prediction_Capstone_Project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkHjKMasF8ZF"
      },
      "source": [
        "# <b><u> Project Title : Predicting whether a customer will default on his/her credit card </u></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDKZBcMGC3b"
      },
      "source": [
        "## <b> Problem Description </b>\n",
        "\n",
        "### This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the [K-S chart](https://www.listendata.com/2019/07/KS-Statistics-Python.html) to evaluate which customers will default on their credit card payments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bVuNrc-GJaj"
      },
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "### <b>Attribute Information: </b>\n",
        "\n",
        "### This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:\n",
        "* ### X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "* ### X2: Gender (1 = male; 2 = female).\n",
        "* ### X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "* ### X4: Marital status (1 = married; 2 = single; 3 = others).\n",
        "* ### X5: Age (year).\n",
        "* ### X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "* ### X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.\n",
        "* ### X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-puVjyI4HS9A"
      },
      "source": [
        "#**Business Objective:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSZtdF-jHcZ8"
      },
      "source": [
        "*  Objective of our project is to predict which customer might default in upcoming months. Before going any further let's have a quick look on defination of what actually meant by Credit Card Default.\n",
        "\n",
        "*  We are all aware what is credit card. It is type of payment payment card in which charges are made against a line of credit instead of the account holder's cash deposits. When someone uses a credit card to make a purchase, that person's account accrues a balance that must be paid off each month.\n",
        "\n",
        "*  Credit card default happens when you have become severely delinquent on your credit card payments.Missing credit card payments once or twice does not count as a default. A payment default occurs when you fail to pay the Minimum Amount Due on the credit card for a few consecutive months.\n",
        "\n",
        "*  So now we know what a credit card is. Now let's see one of problems faced by companies who provide credit cards. Yes it is the peolpe who do not clear off the credit card debt aka credit card defaulters.\n",
        "\n",
        "*  The research aims at developing a mechanism to predict the credit card default beforehand and to identify the potential customer base that can be offered various credit instruments so as to invite minimum default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhWQH4UwHhCU"
      },
      "source": [
        "#**Loading Dataset and Importing Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu6E-ZeNEZEA"
      },
      "outputs": [],
      "source": [
        "#importing required packages\n",
        "\n",
        "import pandas as pd #data processing\n",
        "import numpy as np  #linear algebra\n",
        "\n",
        "#data visualisation\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y20qfukGHvEe"
      },
      "outputs": [],
      "source": [
        "#let's mount the google drive to import the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04EdBdghHxjX"
      },
      "outputs": [],
      "source": [
        "#load the data set from drive\n",
        "path = '/content/drive/MyDrive/Credit Card Default Prediction - Prerit Tyagi/default of credit card clients.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC9izZC0SDZS"
      },
      "outputs": [],
      "source": [
        "credit_df=pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7VY7zG7H6UQ"
      },
      "source": [
        "#**Understand More About The Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1sSnDaNIAVo"
      },
      "source": [
        "### <B>Summary of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEK7SAomH5J9"
      },
      "outputs": [],
      "source": [
        "# Viewing the data of top 5 rows to take a glimps of the data\n",
        "credit_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlqjgVHAIRx8"
      },
      "outputs": [],
      "source": [
        "# View the data of bottom 5 rows to take a glimps of the data\n",
        "credit_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VzYPeVVIVDD"
      },
      "outputs": [],
      "source": [
        "#Getting the shape of dataset with rows and columns\n",
        "print(credit_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S26g3NlPIXzc"
      },
      "outputs": [],
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "credit_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbuLOoszIfmK"
      },
      "outputs": [],
      "source": [
        "#print the unique value\n",
        "credit_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HkIXUN6IibH"
      },
      "outputs": [],
      "source": [
        "#Looking for the description of the dataset to get insights of the data\n",
        "credit_df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld_maSyGIlZu"
      },
      "source": [
        "*  This Dataset contains 30000 lines and 25 columns.\n",
        "*  Default payment next month is our target variable we need to focus on this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aXcMfM_ItnJ"
      },
      "source": [
        "#<b>Features Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA9vknueI7li"
      },
      "source": [
        "Breakdown of Our Features:\n",
        "\n",
        "We have records of 30000 customers. Below are the description of all features we have.\n",
        "\n",
        "*  ID: ID of each client\n",
        "\n",
        "*  LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
        "\n",
        "*  SEX: Gender (1 = male, 2 = female)\n",
        "\n",
        "*  EDUCATION: (1 = graduate school, 2 = university, 3 = high school, 0,4,5,6 = others)\n",
        "\n",
        "*  MARRIAGE: Marital status (0 = others, 1 = married, 2 = single, 3 = others)\n",
        "\n",
        "*  AGE: Age in years\n",
        "\n",
        "Scale for PAY_0 to PAY_6 :\n",
        "\n",
        "(-2 = No consumption, -1 = paid in full, 0 = use of revolving credit (paid minimum only), 1 = payment delay for one month, 2 = payment delay for two months, ... 8 = payment delay for eight months, 9 = payment delay for nine months and above)\n",
        "\n",
        "*  PAY_0: Repayment status in September, 2005 (scale same as above)\n",
        "\n",
        "*  PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "\n",
        "*  PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "\n",
        "*  PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "\n",
        "*  PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "\n",
        "*  PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "\n",
        "*  BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "\n",
        "*  BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "\n",
        "*  BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "\n",
        "*  BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "\n",
        "*  BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "\n",
        "*  BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "\n",
        "*  PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "\n",
        "*  PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "\n",
        "*  PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "\n",
        "*  PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "\n",
        "*  PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "\n",
        "*  PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "\n",
        "*  default.payment.next.month: Default payment (1=yes, 0=no)\n",
        "\n",
        "NOTE:-\n",
        "\n",
        "What do 0 and -2 mean in PAY_X columns?\n",
        "\n",
        "From dataset documentation:\n",
        "\n",
        "PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above). Notice code 0 and 2 are in the PAY_X columns but are not included in the data description. Also by doing the value_counts, it shows 0 is the most frequent observation.\n",
        "\n",
        "-2: No consumption;\n",
        "-1: Paid in full;\n",
        "-0: The use of revolving credit, meaning the payment wasn't due, which makes sense that most customers were using the revolving credit.\n",
        "\n",
        "https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset/discussion/34608\n",
        "\n",
        "In our dataset we got customer credit card transaction history for past 6 month , on basis of which we have to predict if cutomer will default or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G1nocH0JLng"
      },
      "source": [
        "#**Preprocessing the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyj6wbd8JPOR"
      },
      "source": [
        "Why do we need to handle missing values?\n",
        "\n",
        "*  The real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values.that's why we check missing values first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr_SPx1DIi5e"
      },
      "outputs": [],
      "source": [
        "#check for count of missing values in each column.\n",
        "credit_df.isna().sum()\n",
        "credit_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1DIThhRjHzg"
      },
      "outputs": [],
      "source": [
        "#plot the graph to check whether there are any missing value present\n",
        "missing = pd.DataFrame((credit_df.isnull().sum())*100/credit_df.shape[0]).reset_index()\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.pointplot('index',0,color='red',data=missing)\n",
        "plt.xticks(rotation =90,fontsize =7)\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5dxt7esj4_N"
      },
      "source": [
        "As we can see above there are no missing value presents thankfully"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkCsdEgtj9zM"
      },
      "source": [
        "#**Duplicate values**\n",
        "\n",
        "Why is it important to remove duplicate records from my data?\n",
        "\n",
        "*  \"Duplication\" just means that you have repeated data in your dataset. This could be due to things like data entry errors or data collection methods. by removing duplication in our data set, Time and money are saved by not sending identical communications multiple times to the same person."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ds2F_E3jbxp"
      },
      "outputs": [],
      "source": [
        "# Checking Duplicate Values\n",
        "value=len(credit_df[credit_df.duplicated()])\n",
        "print(\"The number of duplicate values in the data set is = \",value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVtcpP01kIln"
      },
      "source": [
        "In the above data after count the missing and duplicate value we came to know that there are no missing and duplicate value present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "834wq-gSkN4F"
      },
      "source": [
        "#**Exploratory Data Analysis**\n",
        "\n",
        "##Importance of EDA?\n",
        "\n",
        "*  An EDA is a thorough examination meant to uncover the underlying structure of a data set and is important for a company because it exposes trends, patterns, and relationships that are not readily apparent.\n",
        "\n",
        "**Univariate Analysis**\n",
        "\n",
        "Why do you do univariate analysis?\n",
        "\n",
        "*  The key objective of Univariate analysis is to simply describe the data to find patterns within the data.\n",
        "\n",
        "**Analysis of Dependent Variable:**\n",
        "\n",
        "What is a dependent variable in data analysis?\n",
        "\n",
        "*  we analyse our dependent variable,A dependent variable is a variable whose value will change depending on the value of another variable.\n",
        "What does 'default.payment.next.month' mean?\n",
        "\n",
        "Is it a predictor or actual data that was collected in the next month - Oct. 2005? According to the reply from the dataset contributor,it seems this is the actual data collected in the next month. \"Y: client's behavior; Y=0 then not default, Y=1 then default\". This information could be used to test the accuracy of the model that will be built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYghN-4EkCBA"
      },
      "outputs": [],
      "source": [
        "#renaming for better convenience\n",
        "credit_df.rename(columns={'default payment next month' : 'default_payment_next_month'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS6YDYnnkqSg"
      },
      "outputs": [],
      "source": [
        "# counts the dependent variable data set\n",
        "credit_df['default_payment_next_month'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmYsXdWbktHZ"
      },
      "outputs": [],
      "source": [
        "# Get the proportion of customers who had default payment in the next month  \n",
        "# About 22% customers had default payment next month\n",
        "\n",
        "credit_df['default_payment_next_month'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyRh0g8Ykwbr"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution \n",
        "#plot the count plot to check the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'default_payment_next_month', data = credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B9MDpMqlD0Y"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "*  0 - Not Default\n",
        "*  1 - Default\n",
        "*  Defaulters are less than the Non Defaulters in the given dataset.\n",
        "\n",
        "As we can see from above graph that both classes are not in proportion and we have imbalanced dataset. we need to do normalize the data in next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE9iCZpYlG5I"
      },
      "source": [
        "#**Analysis of Independent Variable:**\n",
        "\n",
        "**Categorical Features**\n",
        "\n",
        "We have few categorical features in our dataset that are\n",
        "\n",
        "*  sex\n",
        "*  education\n",
        "*  marraige\n",
        "*  age\n",
        "\n",
        "Categorical variables are qualitative data in which the values are assigned to a set of distinct groups or categories. These groups may consist of alphabetic (e.g., male, female) or numeric labels (e.g., male = 0, female = 1) that do not contain mathematical information beyond the frequency counts related to group membership.\n",
        "\n",
        "Let'Check how they are related with out target class.\n",
        "\n",
        "SEX\n",
        "\n",
        "*  1 - Male\n",
        "*  2 - Female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSSgipGOkzgM"
      },
      "outputs": [],
      "source": [
        "# counts the SEX variable data set\n",
        "credit_df['SEX'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrVj1YI0lKeJ"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'SEX', data = credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWs4-7l4lRK8"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "*  1 - Male\n",
        "*  2 - Female\n",
        "\n",
        "Number of Male credit holder is less than Female."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJJBL2cZlW5H"
      },
      "source": [
        "Education\n",
        "\n",
        "*1 = graduate school*\n",
        "*2 = university*\n",
        "*3 = high school*\n",
        "*0 = others*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-6msgcQlNS4"
      },
      "outputs": [],
      "source": [
        "# counts the education  data set variable data set\n",
        "credit_df['EDUCATION'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1k7pC4GlcPX"
      },
      "source": [
        "'EDUCATION' column: notice 5 and 6 are both recorded as 'unknown' and there is 0 which isn't explained in the dataset description. Since the amounts are so small, let's combine 0,4,5,6 to 0 which means\"other'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRc6o-LPlZsU"
      },
      "outputs": [],
      "source": [
        "# Change values 4, 5, 6 to 0 and define 0 as 'others'\n",
        "# 1=graduate school, 2=university, 3=high school, 0=others\n",
        "\n",
        "credit_df[\"EDUCATION\"] = credit_df[\"EDUCATION\"].replace({4:0,5:0,6:0})\n",
        "credit_df[\"EDUCATION\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvgsjVY9lgc7"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'EDUCATION', data = credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAGVyStxloJj"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "*  More number of credit holders are university students followed by Graduates and then High school students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOYL4QCjlqbT"
      },
      "source": [
        "###<b>Marriage\n",
        "\n",
        "####1 = married\n",
        "####2 = single\n",
        "####3= others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwzRQSHnljln"
      },
      "outputs": [],
      "source": [
        "# From dataset description: MARRIAGE: Marital status (1=married, 2=single, 3=others), but there is also 0\n",
        "\n",
        "credit_df[\"MARRIAGE\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wh6dFCAluAd"
      },
      "outputs": [],
      "source": [
        "# counts the education  data set\n",
        "credit_df['MARRIAGE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHnlfn0Slx_g"
      },
      "outputs": [],
      "source": [
        "# How many customers had \"MARRIAGE\" status as 0?\n",
        "\n",
        "credit_df[\"MARRIAGE\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEZlPL-il5NW"
      },
      "source": [
        "'MARRIAGE' column: what does 0 mean in 'MARRIAGE'? Since there are only 0.18% (or 54) observations of 0, we will combine 0 and 3 in one value as 'others'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO1w-t5El09S"
      },
      "outputs": [],
      "source": [
        "# Combine 0 and 3 by changing the value 0 into others\n",
        "\n",
        "credit_df[\"MARRIAGE\"] = credit_df[\"MARRIAGE\"].replace({0:3})\n",
        "credit_df[\"MARRIAGE\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2eZcyYll8Xv"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'MARRIAGE', data = credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfkIA4xlmDHZ"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "* 1 - married\n",
        "* 2 - single\n",
        "* 3 - others\n",
        "More number of credit cards holder are Single"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6iUVnFKmMne"
      },
      "source": [
        "**AGE**\n",
        "\n",
        "Plotting graph of number of ages of all people with credit card irrespective of gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1RqCVwCl_k6"
      },
      "outputs": [],
      "source": [
        "# counts the education  data set\n",
        "credit_df['AGE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zArEbffmaM1"
      },
      "outputs": [],
      "source": [
        "#check the mean of the age group rescpective to the default_payment_next_month\n",
        "credit_df.groupby('default_payment_next_month')['AGE'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETvwnRnY1VGC"
      },
      "outputs": [],
      "source": [
        "credit_df = credit_df.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuoBs8Jg1Ydb"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.countplot(x = 'AGE',data = credit_df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rupKJAj01gP0"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "*  We can see more number of credit cards holder age are between 26-30 years old.\n",
        "\n",
        "*  Age above 60 years old rarely uses the credit card."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sT9IoVO1btd"
      },
      "outputs": [],
      "source": [
        "#plotting the box plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = sns.boxplot(x=\"default_payment_next_month\", y=\"AGE\", data=credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJl0NZKMkH3W"
      },
      "source": [
        "**Numerical features**\n",
        "\n",
        "What is Numerical Data\n",
        "\n",
        "*  Numerical data is a data type expressed in numbers, rather than natural language description. Sometimes called quantitative data, numerical data is always collected in number form. Numerical data differentiates itself from other number form data types with its ability to carry out arithmetic operations with these numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pay9dDQtj9qi"
      },
      "outputs": [],
      "source": [
        "# describe  the limit balance  data set\n",
        "credit_df['LIMIT_BAL'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN_ullVokRPx"
      },
      "outputs": [],
      "source": [
        "#plotting the dist plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(credit_df['LIMIT_BAL'], kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Xyu4h9kaGI"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "*  Maximum amount of given credit in NT dollars is 50,000 followed by 30,000 and 20,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LkT1aS7kjHE"
      },
      "outputs": [],
      "source": [
        "#plotting the bar plot to vizualize the data distribution\n",
        "sns.barplot(x='default_payment_next_month', y='LIMIT_BAL', data=credit_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-M5R9fbpcNW"
      },
      "outputs": [],
      "source": [
        "#plotting the box plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = sns.boxplot(x=\"default_payment_next_month\", y=\"LIMIT_BAL\", data=credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCc2vtAcpjRj"
      },
      "source": [
        "#**Renaming columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uueE1G5pgGY"
      },
      "outputs": [],
      "source": [
        "#renaming columns \n",
        "\n",
        "credit_df.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "credit_df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "credit_df.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPO6gDfupsFm"
      },
      "outputs": [],
      "source": [
        "#check details about the data set\n",
        "credit_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPZNrqIcpua9"
      },
      "outputs": [],
      "source": [
        "credit_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3yP5YPep42J"
      },
      "source": [
        "**Total Bill Amount**\n",
        "\n",
        "What does SNS Pairplot do?\n",
        "\n",
        "*  pairplot. Plot pairwise relationships in a dataset. By default, this function will create a grid of Axes such that each numeric variable in data will by shared across the y-axes across a single row and the x-axes across a single column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5mhGrfapyfS"
      },
      "outputs": [],
      "source": [
        "#assign the bill amount variable to a single variable \n",
        "total_bill_amnt_df = credit_df[['BILL_AMT_SEPT',\t'BILL_AMT_AUG',\t'BILL_AMT_JUL',\t'BILL_AMT_JUN',\t'BILL_AMT_MAY',\t'BILL_AMT_APR']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyFUPLBep91J"
      },
      "outputs": [],
      "source": [
        "#plotting the pair plot for bill amount \n",
        "sns.pairplot(data = total_bill_amnt_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktAWUPKOqGJA"
      },
      "source": [
        "**Previous payment status**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZc5AyR5qAe4"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot for Previous payment status\n",
        "pre_payment = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR']\n",
        "for col in pre_payment:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.countplot(x = col, hue = 'default_payment_next_month', data = credit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3H0HmchqOfG"
      },
      "source": [
        "###**Paid Amount**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6CwcsoWqUEy"
      },
      "outputs": [],
      "source": [
        "#assign the Paid Amount variable to a single variable \n",
        "pay_amnt_df = credit_df[['PAY_AMT_SEPT',\t'PAY_AMT_AUG',\t'PAY_AMT_JUL',\t'PAY_AMT_JUN',\t'PAY_AMT_MAY',\t'PAY_AMT_APR', 'default_payment_next_month']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0Rlu9bIqYBO"
      },
      "outputs": [],
      "source": [
        "#plotting the pair plot for paid amount\n",
        "sns.pairplot(data = pay_amnt_df, hue='default_payment_next_month')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3e9xK4gqfP3"
      },
      "outputs": [],
      "source": [
        "credit_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Bivariate Analysis**\n",
        "\n",
        "what is bivariate analysis\n",
        "\n",
        "*  Bivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association"
      ],
      "metadata": {
        "id": "0L6JueDIErr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sex and default_payment_next_month**"
      ],
      "metadata": {
        "id": "RkWdSK9LEw8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the cat plot to vizualize the data distribution related to the default_payment_next_month\n",
        "x,y = 'SEX', 'default_payment_next_month'\n",
        "\n",
        "(credit_df\n",
        ".groupby(x)[y]\n",
        ".value_counts(normalize=True)\n",
        ".mul(100)\n",
        ".rename('percent')\n",
        ".reset_index()\n",
        ".pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))"
      ],
      "metadata": {
        "id": "cHO11A_tEamV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident from the above graph that the number of defaulter have high proportion of males."
      ],
      "metadata": {
        "id": "4zJNQrtME-_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Education and default_payment_next_month**"
      ],
      "metadata": {
        "id": "0cDLoiiVFB5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the cat plot to vizualize the data distribution related to the default_payment_next_month\n",
        "x,y = 'EDUCATION', 'default_payment_next_month'\n",
        "\n",
        "(credit_df\n",
        ".groupby(x)[y]\n",
        ".value_counts(normalize=True)\n",
        ".mul(100)\n",
        ".rename('percent')\n",
        ".reset_index()\n",
        ".pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))"
      ],
      "metadata": {
        "id": "o5zrlOqeE66q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot it is clear that those people who are other students have higher default payment wrt graduates and university people\n",
        "\n"
      ],
      "metadata": {
        "id": "3PmAasD5FKFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marriage and default_payment_next_month**"
      ],
      "metadata": {
        "id": "QkOC3nicFNHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the cat plot to vizualize the data distribution related to the default_payment_next_month\n",
        "x,y = 'MARRIAGE', 'default_payment_next_month'\n",
        "\n",
        "(credit_df\n",
        ".groupby(x)[y]\n",
        ".value_counts(normalize=True)\n",
        ".mul(100)\n",
        ".rename('percent')\n",
        ".reset_index()\n",
        ".pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))"
      ],
      "metadata": {
        "id": "8L3cAD-_FF8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High defaulter rate when it comes to others"
      ],
      "metadata": {
        "id": "s6MiZwjsFWRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Age and default_payment_next_month**"
      ],
      "metadata": {
        "id": "uTckMWVoFZI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the bar plot to vizualize the data distribution related to the default_payment_next_month\n",
        "plt.figure(figsize=(19,7))\n",
        "sns.barplot(x = 'AGE', y = 'default_payment_next_month', data = credit_df)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vCOKJGp4FQ4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slightly higher defaulter rate in 60's."
      ],
      "metadata": {
        "id": "BEahDODoFoVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Checking of Correlation between variables**\n",
        "\n",
        "*  We check correletion betweeen variables using Correlation heatmap, it is graphical representation of correlation matrix representing correlation between different variables."
      ],
      "metadata": {
        "id": "ftjVoF3MFuU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the heatmap \n",
        "plt.figure(figsize=(20,15))\n",
        "sns.heatmap(credit_df.corr(),annot=True,cmap=\"coolwarm\")"
      ],
      "metadata": {
        "id": "S57G_CQwFjYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems from the above graph is there are some negatively correlated feature like age but we cannot blindly remove this feature because it could be important feature for prediction.\n",
        "\n",
        "ID is unimportant and it has no role in prediction so we will remove it."
      ],
      "metadata": {
        "id": "DIILkQy4F2qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SMOTE**\n",
        "\n",
        "*  In our data set we have Imbalanced Data Distribution in our dependent variable, it generally happens when observations in one of the class are much higher i.e not defaulter or lower than the other classes i.e defaulter.\n",
        "\n",
        "*  As Machine Learning algorithms tend to increase accuracy by reducing the error, they do not consider the class distribution.\n",
        "\n",
        "*  Standard ML techniques such as Decision Tree and Logistic Regression have a bias towards the majority class, and they tend to ignore the minority class. They tend only to predict the majority class, hence, having major misclassification of the minority class in comparison with the majority class. In more technical words, if we have imbalanced data distribution in our dataset then our model becomes more prone to the case when the minority class has a negligible or very lesser recall."
      ],
      "metadata": {
        "id": "IWFlZj5hF8kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SMOTE**\n",
        "\n",
        "*  SMOTE (Synthetic Minority Oversampling Technique) – Oversampling is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them."
      ],
      "metadata": {
        "id": "fKjoMrRiGCIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module \n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "x_smote, y_smote = smote.fit_resample(credit_df.iloc[:,0:-1], credit_df['default_payment_next_month'])\n",
        "\n",
        "print('Original dataset shape', len(credit_df))\n",
        "print('Resampled dataset shape', len(y_smote))"
      ],
      "metadata": {
        "id": "9t3nTyqAFyoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_smote"
      ],
      "metadata": {
        "id": "Q8_qGWkvGFpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(credit_df.columns)"
      ],
      "metadata": {
        "id": "7eyFILB5GId4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns.pop()"
      ],
      "metadata": {
        "id": "qD-tOL_tGLTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance_df = pd.DataFrame(x_smote, columns=columns)"
      ],
      "metadata": {
        "id": "8vAxCKWkGNlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance_df['default_payment_next_month'] = y_smote"
      ],
      "metadata": {
        "id": "RY9qKFtgGQDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the count plot after smote\n",
        "sns.countplot('default_payment_next_month', data = balance_df)"
      ],
      "metadata": {
        "id": "3IicH6kxGSc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance_df[balance_df['default_payment_next_month']==1]"
      ],
      "metadata": {
        "id": "IEzjh3ysGUzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering**"
      ],
      "metadata": {
        "id": "m7PO0U-ZGbxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save a copy file in other variable\n",
        "credit_df_copy = balance_df.copy()"
      ],
      "metadata": {
        "id": "7vCpNnKpGYBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assign the the sum of payment value in one \n",
        "credit_df_copy['total_Payment_Value'] = credit_df_copy['PAY_SEPT'] + credit_df_copy['PAY_AUG'] + credit_df_copy['PAY_JUL'] + credit_df_copy['PAY_JUN'] + credit_df_copy['PAY_MAY'] + credit_df_copy['PAY_APR']"
      ],
      "metadata": {
        "id": "RZ9G8QZGGexi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the correlation by using mean\n",
        "credit_df_copy.groupby('default_payment_next_month')['total_Payment_Value'].mean()"
      ],
      "metadata": {
        "id": "PZiZ_iSAGhUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the box plot \n",
        "plt.figure(figsize=(10,10))\n",
        "sns.boxplot(data = credit_df_copy, x = 'default_payment_next_month', y = 'total_Payment_Value' )"
      ],
      "metadata": {
        "id": "EhC-rNW2Guos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new column due by calculating the sum of total bill amount substract it from the total amont paid\n",
        "credit_df_copy['Dues'] = (credit_df_copy['BILL_AMT_APR']+credit_df_copy['BILL_AMT_MAY']+credit_df_copy['BILL_AMT_JUN']+credit_df_copy['BILL_AMT_JUL']+credit_df_copy['BILL_AMT_SEPT'])-(credit_df_copy['PAY_AMT_APR']+credit_df_copy['PAY_AMT_MAY']+credit_df_copy['PAY_AMT_JUN']+credit_df_copy['PAY_AMT_JUL']+credit_df_copy['PAY_AMT_AUG']+credit_df_copy['PAY_AMT_SEPT'])"
      ],
      "metadata": {
        "id": "TrZTt_IQGxQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the mean to calculate the correletion \n",
        "credit_df_copy.groupby('default_payment_next_month')['Dues'].mean()"
      ],
      "metadata": {
        "id": "M7KbFb54G2lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df_copy.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'},\n",
        "                        'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 0 : 'others'},\n",
        "                        'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others'}}, inplace = True)"
      ],
      "metadata": {
        "id": "bizmN4LvG46j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final data set\n",
        "credit_df_copy.head()"
      ],
      "metadata": {
        "id": "Qs_OmZfpG-E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**One Hot Encoding**\n",
        "*  One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
        "\n",
        "*  here we perform one hot encoding on 'EDUCATION','MARRIAGE','PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR'"
      ],
      "metadata": {
        "id": "qrEKblEzHEiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get dummies \n",
        "credit_df_copy = pd.get_dummies(credit_df_copy,columns=['EDUCATION','MARRIAGE'])"
      ],
      "metadata": {
        "id": "qYLKsYo9HAhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get dummies\n",
        "credit_df_copy = pd.get_dummies(credit_df_copy, columns = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR'], drop_first = True )\n"
      ],
      "metadata": {
        "id": "cj3orx--HIcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LABEL ENCODING FOR SEX\n",
        "encoders_nums = { \"SEX\":{\"FEMALE\": 0, \"MALE\": 1}}\n",
        "credit_df_copy = credit_df_copy.replace(encoders_nums)"
      ],
      "metadata": {
        "id": "s12J6MujHKqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df_copy.drop('ID',axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "BLCVbD05HN_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df_copy.columns"
      ],
      "metadata": {
        "id": "Aeg97aJ7HQGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df_copy.shape"
      ],
      "metadata": {
        "id": "fCIGcMEzHSuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df_copy.head()"
      ],
      "metadata": {
        "id": "aooo71_BHVGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modeling**"
      ],
      "metadata": {
        "id": "VYkZQay_AzG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Performance Metrics**\n",
        "*  **Precision** is a good metric to use when the costs of false positive(FP) is high.\n",
        "\n",
        "    **Precision = TP / (TP + FP)**\n",
        "\n",
        "*  **Recall** is a good metric to use when the cost associated with false negative(FN) is high.\n",
        "\n",
        "    **Recall = TP / (TP + FN)**\n",
        "\n",
        "*  **F1-score** is a weighted average of precision and recall. Thus, it considers FP and FN. This metric is very useful when we have uneven class distribution, as it seeks a balance between precision and recall.\n",
        "\n",
        "    **F1-score = 2 (precision recall) / (precision + recall)**\n",
        "\n",
        "#**Note**\n",
        "*  In this classification problem there is a high cost for the bank when a default credit card is predicted as non-default, since no actions can be taken. Thus, we will give recall more importance ."
      ],
      "metadata": {
        "id": "w26ylPC3A0Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import Packages for performing models**"
      ],
      "metadata": {
        "id": "z-qVnrBvA7bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "from sklearn import metrics  \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve"
      ],
      "metadata": {
        "id": "VkY4g9QGHXXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing Logistic Regression**\n",
        "Logistic Regression is one of the simplest algorithms which estimates the relationship between one dependent binary variable and independent variables, computing the probability of occurrence of an event. The regulation parameter C controls the trade-off between increasing complexity (overfitting) and keeping the model simple (underfitting). For large values of C, the power of regulation is reduced and the model increases its complexity, thus overfitting the data."
      ],
      "metadata": {
        "id": "Xf99yXaxBCn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make a copy\n",
        "credit_df_logistic = credit_df_copy.copy()"
      ],
      "metadata": {
        "id": "IRuiGM4UAwo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the first 5 column\n",
        "credit_df_logistic.head()"
      ],
      "metadata": {
        "id": "USClVCmYBGaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train test split data set"
      ],
      "metadata": {
        "id": "gBIwRkrwBMQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the X and y value\n",
        "X = credit_df_logistic.drop(['default_payment_next_month','total_Payment_Value','Dues'],axis=1)\n",
        "y = credit_df_logistic['default_payment_next_month']"
      ],
      "metadata": {
        "id": "bVXaRyijBJBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = X.columns"
      ],
      "metadata": {
        "id": "8gyKm26aBPVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardise the x value by using satandardscaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "MRnAnVWwBSQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "-wH_jXLCBU-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is GridSearchCV?\n",
        "\n",
        "*  GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit our estimator (model) on your training set. So, in the end, we can select the best parameters from the listed hyperparameters."
      ],
      "metadata": {
        "id": "XKCvoFA-Bbfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the parmeter\n",
        "param_grid = {'penalty':['l1','l2'], 'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000] }"
      ],
      "metadata": {
        "id": "6EKB6Z1xBXSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the parameter \n",
        "grid_lr_clf = GridSearchCV(LogisticRegression(), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_lr_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Gw3Bee8FBfac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_clf = grid_lr_clf.best_estimator_"
      ],
      "metadata": {
        "id": "de6mXz4qBijm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_lr_clf.best_params_"
      ],
      "metadata": {
        "id": "o-tfCewiBmaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_lr_clf.best_score_"
      ],
      "metadata": {
        "id": "MraeL9-RBpS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted Probability\n",
        "train_preds = optimized_clf.predict_proba(X_train)[:,1]\n",
        "test_preds = optimized_clf.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "id": "qmk_VpacBrw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimized_clf.predict(X_train)\n",
        "test_class_preds = optimized_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "4ya81gD8BxWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Evaluation**"
      ],
      "metadata": {
        "id": "KfeBau28B4pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_lr = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_lr = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_lr)\n",
        "print(\"The accuracy on test data is \", test_accuracy_lr)"
      ],
      "metadata": {
        "id": "VM0EcoRVBzym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the accuracy,precission,recall,f1,roc_score \n",
        "test_accuracy_lr = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_lr = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_lr = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_lr = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_lr = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_lr)\n",
        "print(\"The precision on test data is \", test_precision_score_lr)\n",
        "print(\"The recall on test data is \", test_recall_score_lr)\n",
        "print(\"The f1 on test data is \", test_f1_score_lr)\n",
        "print(\"The roc_score on test data is \", test_roc_score_lr)"
      ],
      "metadata": {
        "id": "I197Ns1fCAVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the above evalution we get the results as below\"\n",
        "\n",
        "*  The accuracy on test data is 0.7553984825886778\n",
        "\n",
        "*  The precision on test data is 0.6936446173800259\n",
        "\n",
        "*  The recall on test data is 0.7913583900562297\n",
        "\n",
        "*  The f1 on test data is 0.7392867016864806\n",
        "\n",
        "*  The roc_score on test data is 0.7593522874903104\n",
        "\n",
        "We have implemented logistic regression and we getting f1-sore approx 73%. As we have imbalanced dataset, F1- score is better parameter. Let's go ahead with other models and see if they can yield better result."
      ],
      "metadata": {
        "id": "mbOarktlCHQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Confusion Matrix**\n",
        "\n",
        "Confusion Matrix is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values."
      ],
      "metadata": {
        "id": "2-xChDRrCUEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "cm_lr = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm_lr)"
      ],
      "metadata": {
        "id": "Dido_dVuCDNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_lr, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "YBGP8_PYCsmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Features**\n",
        "Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable."
      ],
      "metadata": {
        "id": "fSxgPnZXC1XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({'Features':columns, 'Importance':np.abs(optimized_clf.coef_).ravel() })"
      ],
      "metadata": {
        "id": "sYDnBbgZCx2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = feature_importance.sort_values(by = 'Importance', ascending=False)[:10]"
      ],
      "metadata": {
        "id": "DkCR9RB5C5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the graph \n",
        "plt.bar(height=feature_importance['Importance'], x= feature_importance['Features'])\n",
        "plt.xticks(rotation=80)\n",
        "plt.title(\"Feature importances via coefficients\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5AOthOfyC8dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above feature importance graph we can say that the most important feature that make an impact on dependt variable are PAY_JUL_1,PAY_MAY_1,PAY_APR_1"
      ],
      "metadata": {
        "id": "8J8m2AceDC47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC AUC curve**\n",
        "\n",
        "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
        "\n",
        "*  True Positive Rate\n",
        "\n",
        "*  False Positive Rate\n",
        "\n",
        "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve"
      ],
      "metadata": {
        "id": "Et7_wVx5DHOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_proba_lr = optimized_clf.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "qJQTgfLCC_kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the graph\n",
        "y_pred_proba = y_preds_proba_lr\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nvz3R-PQDLPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing RandomForest**\n",
        "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting"
      ],
      "metadata": {
        "id": "z4H_D1deDUFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "_YNMxiWFDPdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the X and y value\n",
        "X = credit_df_copy.drop(['default_payment_next_month','total_Payment_Value','Dues'],axis=1)\n",
        "y = credit_df_copy['default_payment_next_month']"
      ],
      "metadata": {
        "id": "zT6g84SWDYbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier()\n",
        "rf_clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "6fWwYHXUDbKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = rf_clf.predict(X_train)\n",
        "test_class_preds = rf_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "rg-dnDXWDfry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Evaluation**"
      ],
      "metadata": {
        "id": "kf3xvOAsEJLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_rf = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_rf)\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)"
      ],
      "metadata": {
        "id": "Ajx-zPy2D-P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_rf = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_rf = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_rf = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_rf = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)\n",
        "print(\"The precision on test data is \", test_precision_score_rf)\n",
        "print(\"The recall on test data is \", test_recall_score_rf)\n",
        "print(\"The f1 on test data is \", test_f1_score_rf)\n",
        "print(\"The roc_score on test data is \", test_roc_score_rf)"
      ],
      "metadata": {
        "id": "JDlFXC7PEPFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from above results that we are getting around 99% train accuracy and 83% for test accuracy which depicts that model is overfitting. However our f1-score is around 82%, which is not bad."
      ],
      "metadata": {
        "id": "92BNfmSFEWCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the parmeter\n",
        "param_grid = {'n_estimators': [100,150,200], 'max_depth': [10,20,30]}"
      ],
      "metadata": {
        "id": "Itw6V_H4ESVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the parameter\n",
        "grid_rf_clf = GridSearchCV(RandomForestClassifier(), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_rf_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8iaFJbYtEZcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_rf_clf.best_score_"
      ],
      "metadata": {
        "id": "F4gsbUzaEcVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_rf_clf.best_params_"
      ],
      "metadata": {
        "id": "ylUi-hL4Eexw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_rf_clf = grid_rf_clf.best_estimator_"
      ],
      "metadata": {
        "id": "3apTYAZZEg5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_rf_clf.predict(X_train)\n",
        "test_class_preds = optimal_rf_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "NNbqbzNxEjcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_rf = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_rf)\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)"
      ],
      "metadata": {
        "id": "O2faqPuYEmfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_rf = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_rf = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_rf = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_rf = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)\n",
        "print(\"The precision on test data is \", test_precision_score_rf)\n",
        "print(\"The recall on test data is \", test_recall_score_rf)\n",
        "print(\"The f1 on test data is \", test_f1_score_rf)\n",
        "print(\"The roc_score on test data is \", test_roc_score_rf)"
      ],
      "metadata": {
        "id": "v92evArgEpvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After gridsearch we getting f1-sore approx 82%. As we have imbalanced dataset, F1- score is better parameter. Let's go ahead with other models and see if they can yield better result"
      ],
      "metadata": {
        "id": "lviZa5uSEu7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Confusion Matrix**\n",
        "Confusion Matrix is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values."
      ],
      "metadata": {
        "id": "06ZnPo7UE_QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "cm_rf = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm_rf)"
      ],
      "metadata": {
        "id": "VT1IntbVEw3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_rf, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "C0LWPV4hFEJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Features**\n",
        "\n",
        "Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "hj5VcucUFKXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(optimal_rf_clf.feature_importances_)"
      ],
      "metadata": {
        "id": "UHRUNURlFHZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "feature_importances_rf = pd.DataFrame(optimal_rf_clf.feature_importances_,\n",
        "                                   index = columns,\n",
        "                                    columns=['importance_rf']).sort_values('importance_rf',\n",
        "                                                                        ascending=False)[:10]\n",
        "                                    \n",
        "plt.subplots(figsize=(17,6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(feature_importances_rf.index, feature_importances_rf['importance_rf'],\n",
        "        color=\"g\",  align=\"center\")\n",
        "plt.xticks(feature_importances_rf.index, rotation = 85)\n",
        "#plt.xlim([-1, X.shape[1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tjQTjPxXFSP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the above feature importance graph we can say that the most important feature that make an impact on dependt variable are LIMIT_BAL,PAY_AMT_SEPT"
      ],
      "metadata": {
        "id": "sufuW5NhTAQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC AUC curve**\n",
        "\n",
        "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
        "\n",
        "*  True Positive Rate\n",
        "\n",
        "*  False Positive Rate\n",
        "\n",
        "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve"
      ],
      "metadata": {
        "id": "_gcEbJM2FcOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_rf_clf.predict(X_train)\n",
        "test_class_preds = optimal_rf_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "84uyx1hYFU_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_proba_rf = optimal_rf_clf.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "6SdLpkMAFgd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "y_pred_proba = y_preds_proba_rf\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ftx8q1ncFjMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing SVC**\n",
        "\n",
        "The Linear Support Vector Classifier (SVC) method applies a linear kernel function to perform classification and it performs well with a large number of samples. If we compare it with the SVC model, the Linear SVC has additional parameters such as penalty normalization which applies 'L1' or 'L2' and loss function."
      ],
      "metadata": {
        "id": "UcHZCdDRTLoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df_svc = credit_df_copy.copy()"
      ],
      "metadata": {
        "id": "qCcJ3s2gFmOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "p34HZzFxTIW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining parameter range \n",
        "param_grid = {'C': [0.1, 1, 10, 100],   \n",
        "              'kernel': ['rbf']}"
      ],
      "metadata": {
        "id": "7Qbex9MhTQ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train-Test split dataset"
      ],
      "metadata": {
        "id": "erqy8_AoTXmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the X and y value\n",
        "X = credit_df_svc.drop(['default_payment_next_month','total_Payment_Value','Dues'],axis=1)\n",
        "y = credit_df_svc['default_payment_next_month']"
      ],
      "metadata": {
        "id": "2v4u_5YoTVsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardise the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Z3Orm-HjTiWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "meoklHrUTlI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV**"
      ],
      "metadata": {
        "id": "wmf_H_hmTqvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the parameter\n",
        "#fit the parameter\n",
        "grid_clf = GridSearchCV(SVC(probability=True), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Zh6lY4DWTngo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6647a10-a9ce-45b0-a739-40284eab10c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_SVC_clf = grid_clf.best_estimator_"
      ],
      "metadata": {
        "id": "W4WHEjxWTt_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_clf.best_params_"
      ],
      "metadata": {
        "id": "gYbn8sYxTwSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_clf.best_score_"
      ],
      "metadata": {
        "id": "rcnRQPHWTzQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_SVC_clf.predict(X_train)\n",
        "test_class_preds = optimal_SVC_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "anXyhExLT1Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "8If77KLST6EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_SVC = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_SVC = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_lr)\n",
        "print(\"The accuracy on test data is \", test_accuracy_lr)"
      ],
      "metadata": {
        "id": "to3zEZRPT3DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_SVC = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_SVC = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_SVC = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_SVC = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_SVC = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_SVC)\n",
        "print(\"The precision on test data is \", test_precision_score_SVC)\n",
        "print(\"The recall on test data is \", test_recall_score_SVC)\n",
        "print(\"The f1 on test data is \", test_f1_score_SVC)\n",
        "print(\"The roc_score on test data is \", test_roc_score_SVC)"
      ],
      "metadata": {
        "id": "_rQdiMMNT8-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from above results that we are getting around 74% train accuracy and 75% for test accuracy which is not bad. But f1- score is 74% approx, so there might be more ground for improvement"
      ],
      "metadata": {
        "id": "AJdDT5UhUCVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "dfmBMByUUFOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "wcYGq0EeT_c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_SVC_clf"
      ],
      "metadata": {
        "id": "LJ8isIUSUIc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_SVC_clf.predict(X_train)\n",
        "test_class_preds = optimal_SVC_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "DMGbeJNrUKnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC AUC curve**"
      ],
      "metadata": {
        "id": "HYjsjPr1UPNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba_SVC = optimal_SVC_clf.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "qMXa--eZUMk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC AUC CURVE\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba_SVC)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_SVC)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ydPSIV2-URh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing XGBoost**\n",
        "XGBoost provides a wrapper class to allow models to be treated like classifiers or regressors in the scikit-learn framework.\n",
        "\n",
        "The XGBoost model for classification is called XGBClassifier. We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function."
      ],
      "metadata": {
        "id": "ZQPTtNV3UW8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import lightgbm and xgboost \n",
        "import lightgbm as lgb \n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "rjsFao8_UTbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The data is stored in a DMatrix object \n",
        "#label is used to define our outcome variable\n",
        "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
        "dtest=xgb.DMatrix(X_test)"
      ],
      "metadata": {
        "id": "0wudvJIHUZ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting parameters for xgboost\n",
        "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}"
      ],
      "metadata": {
        "id": "kJiDAGXHUbwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting parameters for xgboost\n",
        "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}\n",
        "#training our model \n",
        "num_round=50\n",
        "from datetime import datetime \n",
        "start = datetime.now() \n",
        "xg=xgb.train(parameters,dtrain,num_round) \n",
        "stop = datetime.now()"
      ],
      "metadata": {
        "id": "gOvABqFtUeG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execution time of the model \n",
        "execution_time_xgb = stop-start \n",
        "execution_time_xgb"
      ],
      "metadata": {
        "id": "TkGdd3W2UgJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now predicting our model on train set \n",
        "train_class_preds_probs=xg.predict(dtrain) \n",
        "#now predicting our model on test set \n",
        "test_class_preds_probs =xg.predict(dtest)"
      ],
      "metadata": {
        "id": "NL1Mqk6BUib_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_class_preds_probs)"
      ],
      "metadata": {
        "id": "6SIcn4NNUkwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_preds = []\n",
        "test_class_preds = []\n",
        "for i in range(0,len(train_class_preds_probs)):\n",
        "  if train_class_preds_probs[i] >= 0.5:\n",
        "    train_class_preds.append(1)\n",
        "  else:\n",
        "    train_class_preds.append(0)\n",
        "\n",
        "for i in range(0,len(test_class_preds_probs)):\n",
        "  if test_class_preds_probs[i] >= 0.5:\n",
        "    test_class_preds.append(1)\n",
        "  else:\n",
        "    test_class_preds.append(0)"
      ],
      "metadata": {
        "id": "DOirNI7DUmwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_class_preds_probs[:20]"
      ],
      "metadata": {
        "id": "su4NIUzqUpPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_class_preds[:20]"
      ],
      "metadata": {
        "id": "kyMbnZj_Uq_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train)"
      ],
      "metadata": {
        "id": "RSVj1x3jUs3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_class_preds)"
      ],
      "metadata": {
        "id": "-9eDvicnUvJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "4uKVAeL-Uzl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_xgb = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_xgb = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_xgb)\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb)"
      ],
      "metadata": {
        "id": "jNWYrIJwUw9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_xgb = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_xgb = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_xgb = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_xgb = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_xgb = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb)\n",
        "print(\"The precision on test data is \", test_precision_xgb)\n",
        "print(\"The recall on test data is \", test_recall_score_xgb)\n",
        "print(\"The f1 on test data is \", test_f1_score_xgb)\n",
        "print(\"The roc_score on train data is \", test_roc_score_xgb)"
      ],
      "metadata": {
        "id": "yZqXJBU4U3HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "8CG5EcNTU91y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "cm_xg = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm_xg)"
      ],
      "metadata": {
        "id": "vbp7A07EU62b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_xg, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "0SlUj7w7VBMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter Tuning**\n",
        "*  Hyperparameters are crucial as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results.\n",
        "*  We can leverage the maximum power of XGBoost by tuning its hyperparameters"
      ],
      "metadata": {
        "id": "KttPkC6hVPv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from xgboost import  XGBClassifier"
      ],
      "metadata": {
        "id": "sl5AjWnbVEc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the X and y value\n",
        "X = credit_df_copy.drop(['default_payment_next_month','total_Payment_Value','Dues'],axis=1)\n",
        "y = credit_df_copy['default_payment_next_month']"
      ],
      "metadata": {
        "id": "NslOkFkeVSTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "JHGHM2EHVUnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_test1 = {\n",
        " 'max_depth':range(3,10,2),\n",
        " 'min_child_weight':range(1,6,2)\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
        " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
        " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
        " param_grid = param_test1, scoring='accuracy',n_jobs=-1, cv=3, verbose = 2)\n",
        "gsearch1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Er503i60VXRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsearch1.best_score_"
      ],
      "metadata": {
        "id": "hNRo6yJRVa5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_xgb = gsearch1.best_estimator_"
      ],
      "metadata": {
        "id": "QBsK5DaZVdWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_xgb.predict(X_train)\n",
        "test_class_preds = optimal_xgb.predict(X_test)"
      ],
      "metadata": {
        "id": "aNKD5iGNVftJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_xgb_tuned = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_xgb_tuned = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_xgb_tuned)\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb_tuned)"
      ],
      "metadata": {
        "id": "Poepx9goVhi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_xgb_tuned = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_xgb_tuned = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_xgb_tuned = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_xgb_tuned = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_xgb_tuned = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb_tuned)\n",
        "print(\"The precision on test data is \", test_precision_xgb_tuned)\n",
        "print(\"The recall on test data is \", test_recall_score_xgb_tuned)\n",
        "print(\"The f1 on test data is \", test_f1_score_xgb_tuned)\n",
        "print(\"The roc_score on train data is \", test_roc_score_xgb_tuned)"
      ],
      "metadata": {
        "id": "1pxIwNWaVjXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(optimal_xgb.feature_importances_,\n",
        "                                   index = columns,\n",
        "                                    columns=['importance_xgb']).sort_values('importance_xgb',\n",
        "                                                                        ascending=False)[:10]"
      ],
      "metadata": {
        "id": "awNabA5lVlb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Importance**"
      ],
      "metadata": {
        "id": "O1PLUSPcVrDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "feature_importances_xgb = pd.DataFrame(optimal_xgb.feature_importances_,\n",
        "                                   index = columns,\n",
        "                                    columns=['importance_xgb']).sort_values('importance_xgb',\n",
        "                                                                        ascending=False)[:10]\n",
        "                                    \n",
        "plt.subplots(figsize=(17,6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(feature_importances_xgb.index, feature_importances_xgb['importance_xgb'],\n",
        "        color=\"b\",  align=\"center\")\n",
        "plt.xticks(feature_importances_xgb.index, rotation = 85)\n",
        "#plt.xlim([-1, X.shape[1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mk_K69BPVobI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the above feature importance graph we can say that the most important feature that make an impact on dependt variable are PAY_AUG_1"
      ],
      "metadata": {
        "id": "1UI3g_a1Vyp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC_AUC curve**"
      ],
      "metadata": {
        "id": "zrErl6w5V35b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_proba_xgb = optimal_xgb.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "w9dRKEiZVvJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = y_preds_proba_xgb\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6kJJpG-RV-18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q5nkB_UKWAtR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Credit Card Default Prediction - Capstone Project .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6BMxFq8jZBWAA/WnFVUDz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}